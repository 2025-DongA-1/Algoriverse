import pandas as pd
import requests
import re
import pymysql
import time
from tqdm import tqdm
import os
from dotenv import load_dotenv

# .env íŒŒì¼ì— ìˆëŠ” ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤
load_dotenv()

# ==========================================
# [ì„¤ì •] ë³¸ì¸ì˜ DB ì •ë³´ ë° API í‚¤
# ==========================================
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")     
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")   


DB_HOST = os.getenv("DB_HOST")
DB_USER = os.getenv("DB_USER")
DB_PASS = os.getenv("DB_PASS")
DB_NAME = os.getenv("DB_NAME")
DB_PORT = int(os.getenv("DB_PORT"))

# â˜… íŒŒì¼ëª… í™•ì¸ (VS Code ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨)
KEYWORDS_FILE = 'bias_data_final.csv'

# â˜… í•œ í‚¤ì›Œë“œë‹¹ ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜ (ìµœëŒ€ 1000)
# 100ìœ¼ë¡œ ì„¤ì •í•˜ë©´ -> 115ê°œ í‚¤ì›Œë“œ * 100 = 11,500ê°œ (ì•½ 20ë¶„ ì†Œìš”)
# 500ìœ¼ë¡œ ì„¤ì •í•˜ë©´ -> 115ê°œ í‚¤ì›Œë“œ * 500 = 57,500ê°œ (ì•½ 1ì‹œê°„+ ì†Œìš”, 6ê°œì›”ì¹˜ ì¶©ë¶„)
MAX_NEWS_PER_KEYWORD = 500 

def get_naver_news_past(keyword, total_count):
    news_list = []
    # 100ê°œì”© ëŠì–´ì„œ ê³¼ê±° í˜ì´ì§€ë¡œ ë„˜ì–´ê° (Pagination)
    for start_index in range(1, total_count, 100):
        if start_index > 1000: break # ë„¤ì´ë²„ API ìµœëŒ€ í•œê³„

        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
        params = {
            "query": keyword,
            "display": 100,
            "start": start_index, 
            "sort": "sim"  # 'sim'(ì •í™•ë„ìˆœ)ì„ ì“°ë©´ ê³¼ê±°ì˜ ì¤‘ìš”í•œ ê¸°ì‚¬ë„ ì˜ ë‚˜ì˜µë‹ˆë‹¤.
                           # 'date'(ë‚ ì§œìˆœ)ì„ ì“°ë©´ ë¬´ì¡°ê±´ ìµœì‹ ë¶€í„° ê°€ì ¸ì˜µë‹ˆë‹¤.
        }
        try:
            resp = requests.get(url, headers=headers, params=params)
            if resp.status_code == 200:
                items = resp.json().get('items', [])
                if not items: break
                news_list.extend(items)
            else:
                print(f"API Error: {resp.status_code}")
                break
        except Exception as e:
            print(f"Request Error: {e}")
            break
        
        time.sleep(0.3) # API ë³´í˜¸ìš© ë”œë ˆì´
        
    return news_list[:total_count]

def save_bulk_to_db(data_list):
    if not data_list: return
    conn = None
    try:
        conn = pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASS, db=DB_NAME, port=DB_PORT, charset='utf8')
        cur = conn.cursor()
        
        # main.py ê¸°ì¤€ í…Œì´ë¸” ì»¬ëŸ¼ì— ë§ì¶¤
        sql = "INSERT INTO news (category, title, link, description) VALUES (%s, %s, %s, %s)"
        
        success = 0
        # executemanyë¥¼ ì“°ë©´ ë” ë¹ ë¥´ì§€ë§Œ, ì˜¤ë¥˜ í™•ì¸ì„ ìœ„í•´ ë°˜ë³µë¬¸ ì‚¬ìš©
        for item in data_list:
            try:
                cur.execute(sql, (item['category'], item['title'], item['link'], item['description']))
                success += 1
            except:
                pass # ì¤‘ë³µ ê¸°ì‚¬ ë“±ì€ ë¬´ì‹œ
        
        conn.commit()
        # ì¤‘ê°„ ì ê²€ ì¶œë ¥
        if success > 0:
            print(f"   â””â”€ {success}ê±´ ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        print(f"DB Error: {e}")
    finally:
        if conn: conn.close()

def main():
    # 1. í‚¤ì›Œë“œ íŒŒì¼ ë¡œë“œ
    try:
        df = pd.read_csv(KEYWORDS_FILE)
    except:
        df = pd.read_csv(KEYWORDS_FILE, encoding='cp949')

    print(f"ğŸš€ ì´ {len(df)}ê°œì˜ í‚¤ì›Œë“œë¡œ ëŒ€ëŸ‰ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ëª©í‘œ: í‚¤ì›Œë“œë‹¹ {MAX_NEWS_PER_KEYWORD}ê°œ)")
    
    # 2. í‚¤ì›Œë“œë³„ ìˆ˜ì§‘ ë°˜ë³µ
    for i, row in tqdm(df.iterrows(), total=len(df), desc="ì „ì²´ ì§„í–‰ë¥ "):
        keyword = row['keyword']
        category = row['category']
        
        # ê¸°ì‚¬ ìˆ˜ì§‘
        items = get_naver_news_past(keyword, MAX_NEWS_PER_KEYWORD)
        
        # DB ì €ì¥ìš© ë°ì´í„° ê°€ê³µ
        db_data = []
        for item in items:
            title = re.sub(r'<.*?>|&quot;|&gt;|&lt;', '', item['title'])
            desc = re.sub(r'<.*?>|&quot;|&gt;|&lt;', '', item['description'])
            
            db_data.append({
                'category': category,
                'title': title,
                'link': item['originallink'] or item['link'],
                'description': desc
            })
        
        # ë°”ë¡œë°”ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
        save_bulk_to_db(db_data)
        time.sleep(0.5)

    print("\nğŸ‰ 6ê°œì›”ì¹˜(ì¶”ì •) ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ! export_csv.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()