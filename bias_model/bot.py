import time
import schedule
import pymysql
import requests
import re
import pandas as pd
import numpy as np
from numpy.linalg import norm
from analysis_service import BiasAnalyzer # â˜… AI ë‘ë‡Œ íƒ‘ì¬
from datetime import datetime
import os
from dotenv import load_dotenv

# .env íŒŒì¼ì— ìˆëŠ” ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤
load_dotenv()

# ==========================================
# [ì„¤ì •] DB ë° API ì •ë³´
# ==========================================
DB_HOST = os.getenv("DB_HOST")
DB_USER = os.getenv("DB_USER")
DB_PASS = os.getenv("DB_PASS")
DB_NAME = os.getenv("DB_NAME")
DB_PORT = int(os.getenv("DB_PORT"))

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")     
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

CONF_FILE = 'data/bias_data_final.csv'

# ==========================================
# [ì¤€ë¹„] AI ë¶„ì„ê¸° ë¯¸ë¦¬ ë¡œë”© (ë´‡ ì¼œì§ˆ ë•Œ 1ë²ˆë§Œ)
# ==========================================
print("ğŸ¤– ë´‡ ê°€ë™ ì‹œì‘! AI ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
analyzer = BiasAnalyzer()

def get_db_connection():
    return pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASS, db=DB_NAME, port=DB_PORT, charset='utf8')

# ==========================================
# [ìˆ˜ì •] ìŠ¤ë§ˆíŠ¸ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
# ==========================================
def calculate_scores_smart(title, description, search_keyword):
    """
    [ìµœì¢… ê°œì„ ] ê²€ìƒ‰ì–´ ê¸°ë°˜ ê°•ì œ ë¶„ì„ (ìœ íš¨ìœ¨ ëŒ€í­ ìƒìŠ¹)
    """
    # 1. ê¸°ì‚¬ ë²¡í„° ê³„ì‚° (ì´ê±´ ê¸°ë³¸)
    full_text = f"{title} {description}"
    tokens = analyzer.twitter.nouns(full_text)
    valid_tokens = [t for t in tokens if t in analyzer.model.wv and len(t) > 1]
    
    # ê¸°ì‚¬ì— ì“¸ë§Œí•œ ëª…ì‚¬ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ 0ì  (ì´ê±´ ì–´ì©” ìˆ˜ ì—†ìŒ)
    if not valid_tokens: 
        return 0.0, 0.0, None

    article_vec = np.mean([analyzer.model.wv[t] for t in valid_tokens], axis=0)

    # -----------------------------------------------------------
    # 2. ê¸°ì¤€ì (í‚¤ì›Œë“œ) ë²¡í„° ë§Œë“¤ê¸° - ì—¬ê¸°ê°€ í•µì‹¬! âš¡
    # -----------------------------------------------------------
    # ê²€ìƒ‰ì–´(search_keyword)ê°€ ëª¨ë¸ì— ë”± ìˆìœ¼ë©´ ë² ìŠ¤íŠ¸
    if search_keyword in analyzer.model.wv:
        my_vec = analyzer.model.wv[search_keyword]
        
    else:
        # ì—†ìœ¼ë©´? ê²€ìƒ‰ì–´ë¥¼ ìª¼ê°œì„œ ë²¡í„°ë¥¼ ë§Œë“¦ (ì˜ˆ: "4ëŒ€ê°• ë³´ í•´ì²´" -> "4ëŒ€ê°•"+"ë³´"+"í•´ì²´" í‰ê· )
        # 1) ê²€ìƒ‰ì–´ ìì²´ë¥¼ í˜•íƒœì†Œ ë¶„ì„
        kw_tokens = analyzer.twitter.nouns(search_keyword)
        kw_valid = [t for t in kw_tokens if t in analyzer.model.wv]
        
        if kw_valid:
            # ìª¼ê°  ë‹¨ì–´ë“¤ì˜ í‰ê·  ë²¡í„° ì‚¬ìš©
            my_vec = np.mean([analyzer.model.wv[t] for t in kw_valid], axis=0)
        else:
            # ìª¼ê°œë„ ì•„ëŠ” ë‹¨ì–´ê°€ ì—†ìœ¼ë©´... ë¶„ì„ ë¶ˆê°€ (ì–´ì©” ìˆ˜ ì—†ìŒ)
            return 0.0, 0.0, None

    # 3. ë°˜ëŒ€ì–´(Antonym) ë²¡í„° ê°€ì ¸ì˜¤ê¸°
    # (ì´ë¯¸ analysis_serviceì—ì„œ ê³„ì‚°í•´ë‘ )
    if search_keyword in analyzer.antonym_vec_map:
        oppo_vec = analyzer.antonym_vec_map[search_keyword]
    else:
        # ë°˜ëŒ€ì–´ ì„¤ì •ì´ ì•ˆ ëœ í‚¤ì›Œë“œë¼ë©´ ë¶„ì„ ë¶ˆê°€
        return 0.0, 0.0, None

    # 4. ìµœì¢… ì ìˆ˜ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    try:
        sim_cons = np.dot(article_vec, my_vec) / (norm(article_vec)*norm(my_vec))
        sim_prog = np.dot(article_vec, oppo_vec) / (norm(article_vec)*norm(oppo_vec))
        
        # í‚¤ì›Œë“œë¥¼ ì°¾ì•˜ìœ¼ë‹ˆ ê²€ìƒ‰ì–´ë¥¼ ê²°ê³¼ë¡œ ë¦¬í„´
        return float(sim_cons), float(sim_prog), search_keyword
        
    except:
        return 0.0, 0.0, None

# ==========================================
# [ìˆ˜ì •] Job í•¨ìˆ˜ (ì‹¤í–‰ ë¡œì§)
# ==========================================
def job():
    print(f"\nâ° [ìŠ¤ì¼€ì¤„ ì‹¤í–‰] ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘ ({datetime.now()})")
    
    try:
        # DB ì—°ê²° í™•ì¸
        conn = get_db_connection()
        cur = conn.cursor()
        
        # í‚¤ì›Œë“œ íŒŒì¼ ë¡œë“œ
        df_conf = pd.read_csv(CONF_FILE) if 'bias_data_final.csv' in CONF_FILE else pd.read_csv(CONF_FILE, encoding='cp949')
        
        total_collected = 0
        analyzed_count = 0
        
        for i, row in df_conf.iterrows():
            keyword = row['keyword']
            category = row['category']
            
            # API í˜¸ì¶œ (20ê°œ)
            url = "https://openapi.naver.com/v1/search/news.json"
            headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}
            params = {"query": keyword, "display": 20, "sort": "date"}
            
            try:
                resp = requests.get(url, headers=headers, params=params)
                if resp.status_code != 200: continue
                items = resp.json().get('items', [])
            except:
                continue
            
            for item in items:
                title = re.sub(r'<.*?>|&quot;|&gt;|&lt;', '', item['title'])
                desc = re.sub(r'<.*?>|&quot;|&gt;|&lt;', '', item['description'])
                link = item['originallink'] or item['link']
                
                # â˜… ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ì‹¤í–‰
                sim_cons, sim_prog, detected_kw = calculate_scores_smart(title, desc, keyword)
                
                # 1. ì°¨ì´(í¸í–¥ ë ˆë²¨) ê³„ì‚°
                bias_level = sim_cons - sim_prog  # ì´ê²Œ ë°”ë¡œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê·¸ ì ìˆ˜!
                
                # ì ìˆ˜ê°€ 0ì´ë©´ ì¤‘ë¦½, ì•„ë‹ˆë©´ íŒì •
                judgement = 'NEUTRAL'
                if sim_cons != 0 or sim_prog != 0:
                    analyzed_count += 1 # ë¶„ì„ ì„±ê³µ ì¹´ìš´íŠ¸
                    diff = sim_cons - sim_prog
                    if diff > 0.02: judgement = 'CONS'
                    elif diff < -0.02: judgement = 'PROG'
                
                # DB ì €ì¥ (detected_keywords ì»¬ëŸ¼ì— ì‹¤ì œë¡œ ë¶„ì„í•œ ë‹¨ì–´ë¥¼ ë„£ìŒ)
                # ì£¼ì˜: detected_kwê°€ Noneì´ë©´ ì›ë˜ keywordë¥¼ ë„£ìŒ
                final_kw = detected_kw if detected_kw else keyword
                
                sql = """
                    INSERT INTO NEWS_ARTICLES 
                    (category, title, link, description, bias_score_cons, bias_score_prog, bias_level, final_judgement, detected_keywords, created_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
                """
                
                try:
                    cur.execute(sql, (category, title, link, desc, sim_cons, sim_prog, bias_level, judgement, final_kw))
                    total_collected += 1
                except Exception as e:
                    pass # ì¤‘ë³µì€ íŒ¨ìŠ¤
            
            if total_collected % 100 == 0:
                conn.commit()
            
            time.sleep(0.05) 
            
        conn.commit()
        conn.close()
        print(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! (ì´ {total_collected}ê°œ ìˆ˜ì§‘ / ê·¸ ì¤‘ {analyzed_count}ê°œ ìœ íš¨ ë¶„ì„)")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ==========================================
# [ìŠ¤ì¼€ì¤„ë§] ì£¼ê¸° ì„¤ì •
# ==========================================
# í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 10ì´ˆ ë’¤ì— í•œ ë²ˆ ì‹¤í–‰í•˜ê³ , ê·¸ ë’¤ì—” ë§¤ì¼ 3ë²ˆ ì‹¤í–‰
print("â³ ìŠ¤ì¼€ì¤„ëŸ¬ ëŒ€ê¸° ì¤‘... (Ctrl+Cë¡œ ì¢…ë£Œ)")

# (1) ì¦‰ì‹œ ì‹¤í–‰ í™•ì¸ìš©
job()

# (2) ì •í•´ì§„ ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (ì˜ˆ: 6ì‹œê°„ë§ˆë‹¤)
schedule.every(6).hours.do(job)

while True:
    schedule.run_pending()
    time.sleep(1)