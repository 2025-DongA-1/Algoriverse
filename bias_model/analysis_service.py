# ==========================================
# Algoriverse ê´€ì  ë¶„ì„ ì„œë¹„ìŠ¤ (Backend Module)
# ==========================================
import pandas as pd
import numpy as np
from numpy.linalg import norm
from ckonlpy.tag import Twitter
from gensim.models import Word2Vec
import os

# ==========================================
# [ìˆ˜ì •] íŒŒì¼ ê²½ë¡œ ë™ì  ì„¤ì • (í´ë” êµ¬ì¡° ë³€ê²½ ë°˜ì˜)
# ==========================================
# í˜„ìž¬ ì´ íŒŒì¼(analysis_service.py)ì´ ìžˆëŠ” ìœ„ì¹˜ë¥¼ ê¸°ì¤€ì ìœ¼ë¡œ ìž¡ìŠµë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ ì–´ë””ì„œ ì‹¤í–‰í•˜ë“  ê²½ë¡œ ì—ëŸ¬ê°€ ì ˆëŒ€ ì•ˆ ë‚©ë‹ˆë‹¤!
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# models í´ë” ì•ˆì˜ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
MODEL_PATH = os.path.join(BASE_DIR, 'models', 'algoriverse.model')

# data í´ë” ì•ˆì˜ CSV íŒŒì¼ ê²½ë¡œ
CONF_PATH = os.path.join(BASE_DIR, 'data', 'bias_data_final.csv')

class BiasAnalyzer:
    def __init__(self):
        print("ðŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘... (ìž ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)")
        
        # 1. ëª¨ë¸ íŒŒì¼ ì¡´ìž¬ í™•ì¸
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì˜ˆìƒ ê²½ë¡œ: {MODEL_PATH}")
            
        # 2. Word2Vec ëª¨ë¸ ë¡œë“œ
        self.model = Word2Vec.load(MODEL_PATH)
        
        # 3. ì„¤ì • íŒŒì¼(CSV) ë¡œë“œ
        if not os.path.exists(CONF_PATH):
             raise FileNotFoundError(f"âŒ ì„¤ì • íŒŒì¼(CSV)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nì˜ˆìƒ ê²½ë¡œ: {CONF_PATH}")

        try:
            self.df_conf = pd.read_csv(CONF_PATH)
        except:
            self.df_conf = pd.read_csv(CONF_PATH, encoding='cp949')
            
        # 4. í˜•íƒœì†Œ ë¶„ì„ê¸°(Twitter) & ì‚¬ìš©ìž ì‚¬ì „ êµ¬ì¶•
        self.twitter = Twitter()
        self.antonym_vec_map = {}
        
        self._initialize_dictionary()
        print("âœ… AI ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ!")

    def _initialize_dictionary(self):
        """í‚¤ì›Œë“œ ì‚¬ì „ì„ ë“±ë¡í•˜ê³  ë°˜ëŒ€ì–´ ë²¡í„°ë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        
        # (1) í‚¤ì›Œë“œì™€ ë°˜ëŒ€ì–´ë¥¼ ì‚¬ì „ì— ê°•ì œ ë“±ë¡ (ìª¼ê°œì§ ë°©ì§€)
        new_words = self.df_conf['keyword'].tolist()
        for _, row in self.df_conf.iterrows():
            if pd.notna(row['antonym']):
                ants = str(row['antonym']).split(',')
                new_words.extend([a.strip() for a in ants])
        
        # ì¤‘ë³µ ì œê±° í›„ ë“±ë¡
        for word in set(new_words):
            self.twitter.add_dictionary(word, 'Noun')

        # (2) ë°˜ëŒ€ì–´ ë²¡í„°(ê¸°ì¤€ì ) ë¯¸ë¦¬ ê³„ì‚°
        for _, row in self.df_conf.iterrows():
            target = row['keyword']
            if pd.notna(row['antonym']):
                raw_ants = [a.strip() for a in str(row['antonym']).split(',')]
                valid_vecs = []
                
                for ant in raw_ants:
                    # ëª¨ë¸ì— í†µì§¸ë¡œ ìžˆìœ¼ë©´ ì‚¬ìš©
                    if ant in self.model.wv:
                        valid_vecs.append(self.model.wv[ant])
                    else:
                        # ì—†ìœ¼ë©´ í˜•íƒœì†Œ ë¶„ì„ í›„ í‰ê· ê°’ ì‚¬ìš©
                        tokens = self.twitter.nouns(ant)
                        sub_vecs = [self.model.wv[t] for t in tokens if t in self.model.wv]
                        if sub_vecs:
                            valid_vecs.append(np.mean(sub_vecs, axis=0))
                
                # ìœ íš¨í•œ ë°˜ëŒ€ì–´ ë²¡í„°ë“¤ì˜ í‰ê· ì„ ì €ìž¥
                if valid_vecs:
                    self.antonym_vec_map[target] = np.mean(valid_vecs, axis=0)

    def analyze_article(self, title, description, target_keyword):
        """
        ê¸°ì‚¬ ì œëª©ê³¼ ìš”ì•½ì„ ë°›ì•„ íŽ¸í–¥ë„ ì ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
        Return: ì–‘ìˆ˜(+)ë©´ í•´ë‹¹ í‚¤ì›Œë“œ ì„±í–¥, ìŒìˆ˜(-)ë©´ ë°˜ëŒ€ ì„±í–¥
        """
        # 1. ë¶„ì„ ê°€ëŠ¥í•œ í‚¤ì›Œë“œì¸ì§€ ì²´í¬
        if target_keyword not in self.model.wv or target_keyword not in self.antonym_vec_map:
            return None # ë¶„ì„ ë¶ˆê°€ (ë°ì´í„° ë¶€ì¡±)

        # 2. ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        full_text = f"{title} {description}"
        tokens = self.twitter.nouns(full_text)
        
        # ì˜ë¯¸ ìžˆëŠ” ë‹¨ì–´ë§Œ í•„í„°ë§
        valid_tokens = [t for t in tokens if t in self.model.wv and len(t) > 1]
        
        if not valid_tokens: return 0.0

        # 3. ë²¡í„° ê³„ì‚° (Core Logic)
        my_vec = self.model.wv[target_keyword]          # ê¸°ì¤€ì  (ì˜ˆ: ê±´êµ­ì ˆ)
        oppo_vec = self.antonym_vec_map[target_keyword] # ë°˜ëŒ€ì  (ì˜ˆ: ê´‘ë³µì ˆ, ë…ë¦½ìš´ë™)
        article_vec = np.mean([self.model.wv[t] for t in valid_tokens], axis=0) # ê¸°ì‚¬ ìœ„ì¹˜
        
        # 4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ
        # (ë‚˜ëž‘ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ê°€) - (ë°˜ëŒ€ëž‘ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ê°€)
        sim_my = np.dot(article_vec, my_vec) / (norm(article_vec)*norm(my_vec))
        sim_oppo = np.dot(article_vec, oppo_vec) / (norm(article_vec)*norm(oppo_vec))
        
        # ì ìˆ˜ ë¦¬í„´
        return sim_my - sim_oppo

# ==========================================
# [ì‹¤í–‰ í…ŒìŠ¤íŠ¸] í„°ë¯¸ë„ì—ì„œ python analysis_service.py ì‹¤í–‰ ì‹œ ìž‘ë™
# ==========================================
if __name__ == "__main__":
    # 1. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    analyzer = BiasAnalyzer()
    
    # 2. ê°€ìƒì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_keyword = "ê±´êµ­ì ˆ"
    test_title = "ê´‘ë³µíšŒ, 'ê±´êµ­ì ˆ ì œì •ì€ ë§¤êµ­ í–‰ìœ„' ê°•ë ¥ ê·œíƒ„"
    test_desc = "ë‰´ë¼ì´íŠ¸ ì—­ì‚¬ê´€ì— ê¸°ë°˜í•œ ê±´êµ­ì ˆ ì¶”ì§„ì„ ì¦‰ê° ì¤‘ë‹¨í•˜ë¼."
    
    print(f"\nðŸ”Ž ê²€ìƒ‰ í‚¤ì›Œë“œ: {test_keyword}")
    print(f"ðŸ“„ ê¸°ì‚¬ ì œëª©: {test_title}")
    
    # 3. ë¶„ì„ ìˆ˜í–‰
    score = analyzer.analyze_article(test_title, test_desc, test_keyword)
    
    if score is None:
        print("âŒ ë¶„ì„ ë¶ˆê°€ (í‚¤ì›Œë“œ ë°ì´í„° ë¶€ì¡±)")
    else:
        print(f"ðŸ“Š ë¶„ì„ ì ìˆ˜: {score:.4f}")
        if score > 0:
            print("ðŸ‘‰ ê²°ê³¼: [ë³´ìˆ˜/ê±´êµ­ì ˆ ì˜¹í˜¸] ì„±í–¥")
        else:
            print("ðŸ‘‰ ê²°ê³¼: [ì§„ë³´/ê±´êµ­ì ˆ ë°˜ëŒ€] ì„±í–¥")