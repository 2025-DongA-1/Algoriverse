import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification

MODEL_PATH = "./my_bias_model"

print(f"ğŸ“‚ AI ëª¨ë¸ ë¡œë”© ì¤‘... ({MODEL_PATH})")
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    model.eval()
    print("âœ… AI ë¡œë”© ì™„ë£Œ!")
except Exception as e:
    print(f"ğŸš¨ ë¡œë”© ì‹¤íŒ¨: {e}")
    exit()

# ğŸ”¥ [ìˆ˜ì •ë¨] ì œëª©(title)ê³¼ ë³¸ë¬¸(content)ì„ ê°™ì´ ë°›ìŠµë‹ˆë‹¤.
def get_bias(title, content):
    # 1. ëª¨ë¸ì´ ê¸€ì„ ë‹¤ ëª» ì½ìœ¼ë¯€ë¡œ, ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ ì˜ë¼ì„œ í•©ì¹˜ê¸° ì „ëµ!
    # ì œëª© + ë³¸ë¬¸ ì• 150ì + ë³¸ë¬¸ ë’¤ 150ì (ë³´í†µ ì£¼ì¥ì€ ëì— ìˆìŒ)
    if len(content) > 300:
        processed_content = title + " " + content[:150] + " ... " + content[-150:]
    else:
        processed_content = title + " " + content

    inputs = tokenizer(
        processed_content, 
        return_tensors="pt", 
        truncation=True, 
        padding=True, 
        max_length=512  # ëª¨ë¸ì´ í—ˆìš©í•˜ëŠ” ìµœëŒ€ ê¸¸ì´
    )
    
    # 3. ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(**inputs)
        probs = F.softmax(outputs.logits, dim=-1)

    prob_liberal = probs[0][0].item() * 100
    prob_conservative = probs[0][1].item() * 100
    
    if prob_conservative > prob_liberal:
        return "ë³´ìˆ˜", round(prob_conservative, 2)
    else:
        return "ì§„ë³´", round(prob_liberal, 2)

# =========================================================
# í…ŒìŠ¤íŠ¸
# =========================================================
if __name__ == "__main__":
    print("\n[ ğŸ“¢ ê³ ì„±ëŠ¥ ì •ì¹˜ ì„±í–¥ ë¶„ì„ê¸° (ë³¸ë¬¸ í¬í•¨) ]")
    
    # ì˜ˆì‹œ 1: ì œëª©ì€ ì• ë§¤í•˜ì§€ë§Œ ë³¸ë¬¸ì´ ì§„ë³´ì¸ ê²½ìš°
    t1 = "ì •ë¶€, ìƒˆë¡œìš´ ì„¸ì œ ê°œí¸ì•ˆ ë°œí‘œ"
    c1 = "ì´ë²ˆ ê°œí¸ì•ˆì€ ì‚¬ì‹¤ìƒ ë¶€ì ê°ì„¸ë¼ëŠ” ë¹„íŒì„ í”¼í•˜ê¸° ì–´ë µë‹¤. ì„œë¯¼ë“¤ì˜ í˜œíƒì€ ì¤„ì–´ë“¤ê³  ëŒ€ê¸°ì—… í˜œíƒë§Œ ëŠ˜ì–´ë‚¬ë‹¤."
    res1, score1 = get_bias(t1, c1)
    print(f"\nê¸°ì‚¬1: {t1}\níŒë…: {res1} ({score1}%)")
    
    # ì˜ˆì‹œ 2: ì œëª©ì€ í‰ë²”í•˜ì§€ë§Œ ë³¸ë¬¸ì´ ë³´ìˆ˜ì¸ ê²½ìš°
    t2 = "ì—ë„ˆì§€ ì •ì±…, ë‹¤ì‹œ ì›ì ìœ¼ë¡œ"
    c2 = "ì§€ë‚œ ì •ë¶€ì˜ íƒˆì›ì „ ì •ì±…ìœ¼ë¡œ í•œì „ ì ìê°€ ì‹¬ê°í•˜ë‹¤. ì›ì „ ìƒíƒœê³„ë¥¼ ë³µì›í•˜ì—¬ ì—ë„ˆì§€ ì•ˆë³´ë¥¼ ì§€ì¼œì•¼ í•œë‹¤."
    res2, score2 = get_bias(t2, c2)
    print(f"\nê¸°ì‚¬2: {t2}\níŒë…: {res2} ({score2}%)")

    # ì§ì ‘ ì…ë ¥ ëª¨ë“œ
    while True:
        print("\n" + "-"*30)
        user_title = input("ğŸ“ ì œëª© ì…ë ¥ (ì¢…ë£Œ: q): ")
        if user_title.lower() == 'q': break
        user_content = input("ğŸ“ ë³¸ë¬¸ ì…ë ¥ (ì—†ìœ¼ë©´ ì—”í„°): ")
        
        result, score = get_bias(user_title, user_content)
        print(f"ğŸ‘‰ ê²°ê³¼: {result} (í™•ì‹ ë„: {score}%)")