import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from tqdm import tqdm

# ==============================================================================
# [ì„¤ì •] ì§‘ì—ì„œ ëŒë¦´ ë•ŒëŠ” ë„‰ë„‰í•˜ê²Œ ê¸ì–´ë„ ë©ë‹ˆë‹¤!
KEYWORDS = ["ê¹€ê±´í¬ íŠ¹ê²€", "ì±„ìƒë³‘ íŠ¹ê²€", "ê¸ˆíˆ¬ì„¸ íì§€", "ì˜ëŒ€ ì¦ì›", "íƒˆì›ì „", "ì´ì¬ëª… ì¬íŒ", "ê²€ìˆ˜ì™„ë°•"]
PAGES_PER_KEYWORD = 5  # í‚¤ì›Œë“œë‹¹ 5í˜ì´ì§€ (ì•½ 50ê°œì”©)

# [ì–¸ë¡ ì‚¬ ë§¤í•‘]
PRESS_MAP = {
    "ì¡°ì„ ": 1, "ì¤‘ì•™": 1, "ë™ì•„": 1, "ë¬¸í™”": 1, "í•œêµ­ê²½ì œ": 1, "ë§¤ì¼ê²½ì œ": 1, "ë°ì¼ë¦¬ì•ˆ": 1,
    "í•œê²¨ë ˆ": 0, "ê²½í–¥": 0, "ì˜¤ë§ˆì´": 0, "í”„ë ˆì‹œì•ˆ": 0, "ë¯¸ë””ì–´ì˜¤ëŠ˜": 0, "ë…¸ì»·": 0
}
# ==============================================================================

headers = {
    # ë¡œë´‡ì´ ì•„ë‹Œ ì²™ ìœ„ì¥í•˜ëŠ” ì£¼ë¯¼ë“±ë¡ì¦
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

results = []

print(f"ğŸš€ [ë¡œì»¬ PC ë²„ì „] í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

for keyword in KEYWORDS:
    search_query = keyword + " ì‚¬ì„¤"
    print(f"\nğŸ” ê²€ìƒ‰ì–´: {search_query}")
    
    for page in tqdm(range(PAGES_PER_KEYWORD)):
        start = page * 10 + 1
        url = f"https://search.naver.com/search.naver?where=news&query={search_query}&start={start}"
        
        try:
            res = requests.get(url, headers=headers)
            soup = BeautifulSoup(res.text, 'html.parser')
            articles = soup.select("li.bx") 
            
            if not articles:
                continue

            for article in articles:
                # 1. ì–¸ë¡ ì‚¬ í™•ì¸
                press_tag = article.select_one("a.info.press")
                if not press_tag: continue
                press_name = press_tag.get_text(strip=True).replace("ì–¸ë¡ ì‚¬ ì„ ì •", "")
                
                matched_label = None
                for key, label in PRESS_MAP.items():
                    if key in press_name:
                        matched_label = label
                        break
                
                if matched_label is None: continue
                
                # 2. ì œëª© ê°€ì ¸ì˜¤ê¸°
                title_tag = article.select_one("a.news_tit")
                if not title_tag: continue
                title = title_tag.get_text(strip=True)
                
                # 3. ë³¸ë¬¸ ì‹œë„
                content = ""
                links = article.select("a.info")
                naver_link = None
                for link in links:
                    if "n.news.naver.com" in link.get('href', ''):
                        naver_link = link['href']
                        break
                
                if naver_link:
                    try:
                        sub_res = requests.get(naver_link, headers=headers)
                        sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                        body = sub_soup.select_one("#dic_area") or sub_soup.select_one("#newsct_article")
                        if body:
                            content = body.get_text(strip=True)
                    except:
                        pass
                
                if not content or len(content) < 10:
                    dsc = article.select_one("div.dsc_wrap")
                    if dsc: content = dsc.get_text(strip=True)
                    else: content = title 
                
                results.append({
                    "title": title,
                    "content": content,
                    "labels": matched_label
                })
                
            # [ì¤‘ìš”] ë„¤ì´ë²„ê°€ ëˆˆì¹˜ì±„ì§€ ëª»í•˜ê²Œ ëœë¤í•˜ê²Œ ì‰½ë‹ˆë‹¤ (0.5ì´ˆ ~ 1.5ì´ˆ)
            time.sleep(random.uniform(0.5, 1.5))
                
        except Exception as e:
            print(f"Error: {e}")
            continue

# ì €ì¥
if len(results) > 0:
    df = pd.DataFrame(results)
    df = df.drop_duplicates(subset=['title'])
    file_name = "political_auto_data.csv"
    df.to_csv(file_name, index=False, encoding="utf-8-sig")
    print(f"\nğŸ‰ ì„±ê³µ! ì´ {len(df)}ê°œ ì €ì¥ë¨: {file_name}")
    print(df['labels'].value_counts())
else:
    print("\nğŸ˜­ ë¡œì»¬ì—ì„œë„ ì•ˆ ë˜ë©´ ë„¤ì´ë²„ê°€ ì •ë§ ê¹ê¹í•œ ê²ë‹ˆë‹¤.")