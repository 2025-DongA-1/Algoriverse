import os
import time
import random
import schedule
import pymysql
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from predict import get_bias

load_dotenv()

DB_CONFIG = {
    'host': os.getenv("DB_HOST"),
    'user': os.getenv("DB_USER"),
    'password': os.getenv("DB_PASS"),
    'db': os.getenv("DB_NAME"),
    'port': int(os.getenv("DB_PORT")),
    "charset": "utf8mb4",
    "cursorclass": pymysql.cursors.DictCursor
}

def job():
    keywords = ["ê²€ì°°ê°œí˜","ê³µìˆ˜ì²˜","ë…¸ë€ë´‰íˆ¬ë²•","íƒˆì›ì „", "ëŒ€ë¶ì •ì±…" ] 
    print(f"\nâ° [Auto System] ì •ê¸° ì‘ì—… ì‹œì‘: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    for keyword in keywords:
        # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 20ê°œê¹Œì§€ ìˆ˜ì§‘í•˜ë„ë¡ ì„¤ì •
        crawl_and_analyze(keyword, limit=20)
        
    print(f"ğŸ’¤ ì‘ì—… ì™„ë£Œ. ë‹¤ìŒ ìŠ¤ì¼€ì¤„ ëŒ€ê¸° ì¤‘... (4ì‹œê°„ ë’¤ ì‹¤í–‰)\n")

def crawl_and_analyze(keyword, limit=20):
    print(f"ğŸš€ '{keyword}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (ëª©í‘œ: {limit}ê°œ)...")

    chrome_options = Options()
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # chrome_options.add_argument("--headless") # ì„œë²„ìš©
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36")
    
    driver = webdriver.Chrome(options=chrome_options)
    conn = None

    try:
        url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort=1" 
        # &sort=1 ì¶”ê°€: 'ìµœì‹ ìˆœ'ìœ¼ë¡œ ì •ë ¬ (ì¤‘ë³µ ì¤„ì´ê³  ìƒˆ ê¸°ì‚¬ ì°¾ê¸°ì— ìœ ë¦¬í•¨)
        
        driver.get(url)
        time.sleep(2)

        # ğŸ”¥ [ì¶”ê°€ëœ ê¸°ëŠ¥] ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì„œ ê¸°ì‚¬ë¥¼ ë” ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        # ë„¤ì´ë²„ëŠ” ìŠ¤í¬ë¡¤ì„ ë‚´ë ¤ì•¼ ë‹¤ìŒ ê¸°ì‚¬ë“¤ì´ ë¡œë”©ë©ë‹ˆë‹¤.
        for _ in range(3): # 3ë²ˆ ì •ë„ ë‚´ë¦¬ë©´ 30~40ê°œ ì •ë„ ë¡œë”©ë¨
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1.5)

        # ë§í¬ ìˆ˜ì§‘
        naver_links = []
        elements = driver.find_elements(By.CSS_SELECTOR, "a[href*='n.news.naver.com']")
        
        for elm in elements:
            link = elm.get_attribute("href")
            if "n.news.naver.com/mnews/article" in link and link not in naver_links:
                naver_links.append(link)
        
        # ì„¤ì •í•œ ê°œìˆ˜(limit)ë§Œí¼ë§Œ ìë¥´ê¸°
        target_links = naver_links[:limit]
        print(f"ğŸ¯ ë°œê²¬ëœ ë„¤ì´ë²„ ë‰´ìŠ¤: ì´ {len(naver_links)}ê°œ -> ìƒìœ„ {len(target_links)}ê°œ ë¶„ì„ ì‹œë„")

        conn = pymysql.connect(**DB_CONFIG)
        cursor = conn.cursor()

        new_article_count = 0

        for link in target_links:
            try:
                # ì¤‘ë³µ ì²´í¬
                check_sql = "SELECT id FROM NEWS_ARTICLES WHERE link = %s"
                cursor.execute(check_sql, (link,))
                if cursor.fetchone():
                    # ì´ë¯¸ ìˆëŠ” ê¸°ì‚¬ëŠ” ì¡°ìš©íˆ ë„˜ì–´ê° (ë¡œê·¸ ë„ˆë¬´ ë§ì´ ì°íˆëŠ” ê²ƒ ë°©ì§€)
                    continue

                driver.get(link)
                # ğŸ”¥ [ìˆ˜ì •] ëœë¤í•˜ê²Œ 1.5ì´ˆ ~ 3.5ì´ˆ ì‚¬ì´ ì‰¬ê¸° (ì‚¬ëŒì²˜ëŸ¼ ë³´ì„)
                time.sleep(random.uniform(1.5, 3.5))

                try:
                    title = driver.find_element(By.CSS_SELECTOR, "meta[property='og:title']").get_attribute("content")
                except:
                    title = driver.title

                try:
                    content = driver.find_element(By.ID, "dic_area").text
                except:
                    continue # ë³¸ë¬¸ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
                
                if len(content) < 50: continue

                # AI ë¶„ì„
                bias_label, bias_score = get_bias(title, content)
                
                print(f"   ğŸ†• [ì‹ ê·œ] {bias_label}: {title[:10]}...")

                insert_sql = """
                    INSERT INTO NEWS_ARTICLES 
                    (keyword, title, content, link, bias, bias_score)
                    VALUES (%s, %s, %s, %s, %s, %s)
                """
                cursor.execute(insert_sql, (keyword, title, content, link, bias_label, bias_score))
                conn.commit()
                new_article_count += 1

            except Exception as e:
                continue

        print(f"âœ¨ '{keyword}' ì²˜ë¦¬ ì™„ë£Œ: ì‹ ê·œ ì €ì¥ {new_article_count}ê±´")

    except Exception as e:
        print(f"ğŸš¨ ì—ëŸ¬: {e}")
    finally:
        driver.quit()
        if conn: conn.close()

if __name__ == "__main__":
    print("ğŸš€ ì‹œìŠ¤í…œ ê°€ë™ (4ì‹œê°„ ì£¼ê¸° / ìµœì‹ ìˆœ ì •ë ¬ / 20ê°œ ìˆ˜ì§‘)")
    
    job() # ì‹œì‘í•˜ìë§ˆì 1íšŒ ì‹¤í–‰

    # â° [ë³€ê²½] 4ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
    schedule.every(4).hours.do(job)

    # (ì˜µì…˜) ë§Œì•½ íŠ¹ì • ì‹œê°„ì—ë§Œ í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì²˜ëŸ¼ ì“°ì„¸ìš”
    # schedule.every().day.at("09:00").do(job)
    # schedule.every().day.at("18:00").do(job)

    while True:
        schedule.run_pending()
        time.sleep(1)